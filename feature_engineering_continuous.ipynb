{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "## List All Feature Views"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "# Feature Engineering with Snowflake Feature Store\n",
    "\n",
    "This notebook derives features from raw continuous customer data using **Snowflake Feature Store**.\n",
    "\n",
    "The Feature Store provides:\n",
    "- Centralized feature definitions and management\n",
    "- Automatic feature refresh on schedule\n",
    "- Point-in-time correct features for training\n",
    "- Feature lineage and discovery\n",
    "- Consistent features for training and inference\n",
    "\n",
    "**Prerequisites**: Run `generate_continuous_data.ipynb` first to create raw data tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from snowflake.snowpark.context import get_active_session\n",
    "from snowflake.snowpark import functions as F, Window\n",
    "from snowflake.ml.feature_store import (\n",
    "    FeatureStore,\n",
    "    FeatureView,\n",
    "    Entity,\n",
    "    CreationMode\n",
    ")\n",
    "\n",
    "session = get_active_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATABASE = 'ML_DEMO'\n",
    "SCHEMA = 'PUBLIC'\n",
    "FEATURE_STORE_NAME = 'CLV_FEATURE_STORE'\n",
    "WAREHOUSE = 'ML_DEMO_WH'\n",
    "\n",
    "session.use_database(DATABASE)\n",
    "session.use_schema(SCHEMA)\n",
    "session.use_warehouse(WAREHOUSE)\n",
    "\n",
    "OBSERVATION_DATE = datetime(2024, 6, 30)\n",
    "\n",
    "print(f\"Database: {DATABASE}\")\n",
    "print(f\"Schema: {SCHEMA}\")\n",
    "print(f\"Feature Store: {FEATURE_STORE_NAME}\")\n",
    "print(f\"Warehouse: {WAREHOUSE}\")\n",
    "print(f\"Observation Date: {OBSERVATION_DATE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## Create or Connect to Feature Store\n",
    "\n",
    "A feature store in Snowflake is a schema that contains feature views (backed by dynamic tables or views)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = FeatureStore(\n",
    "    session=session,\n",
    "    database=DATABASE,\n",
    "    name=FEATURE_STORE_NAME,\n",
    "    default_warehouse=WAREHOUSE,\n",
    "    creation_mode=CreationMode.CREATE_IF_NOT_EXIST\n",
    ")\n",
    "\n",
    "print(f\"✓ Feature Store ready: {DATABASE}.{FEATURE_STORE_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## Register Entity\n",
    "\n",
    "Entities organize features by subject. Here we create a CUSTOMER entity with CUSTOMER_ID as the join key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    customer_entity = fs.get_entity(\"CUSTOMER\")\n",
    "    print(\"✓ CUSTOMER entity already exists\")\n",
    "except:\n",
    "    customer_entity = Entity(\n",
    "        name=\"CUSTOMER\",\n",
    "        join_keys=[\"CUSTOMER_ID\"],\n",
    "        desc=\"Customer entity for CLV prediction\"\n",
    "    )\n",
    "    fs.register_entity(customer_entity)\n",
    "    print(\"✓ Created CUSTOMER entity\")\n",
    "\n",
    "print(f\"  Join keys: {customer_entity.join_keys}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## Feature View 1: RFM Features\n",
    "\n",
    "**RFM** (Recency, Frequency, Monetary) features capture customer purchase behavior:\n",
    "- **Recency**: Days since last purchase\n",
    "- **Frequency**: Total number of purchases  \n",
    "- **Monetary**: Total and average spending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions_df = session.table(f\"{DATABASE}.{SCHEMA}.CONTINUOUS_TRANSACTIONS\")\n",
    "\n",
    "rfm_df = transactions_df.group_by(\"CUSTOMER_ID\").agg([\n",
    "    F.datediff(\n",
    "        \"day\",\n",
    "        F.max(\"TRANSACTION_DATE\"),\n",
    "        F.lit(OBSERVATION_DATE)\n",
    "    ).alias(\"RECENCY_DAYS\"),\n",
    "    F.count(\"TRANSACTION_ID\").alias(\"FREQUENCY\"),\n",
    "    F.sum(\"AMOUNT\").alias(\"MONETARY_TOTAL\"),\n",
    "    F.avg(\"AMOUNT\").alias(\"MONETARY_AVG\"),\n",
    "    F.min(\"TRANSACTION_DATE\").alias(\"FIRST_PURCHASE_DATE\"),\n",
    "    F.max(\"TRANSACTION_DATE\").alias(\"LAST_PURCHASE_DATE\")\n",
    "])\n",
    "\n",
    "rfm_df = rfm_df.with_column(\n",
    "    \"CUSTOMER_TENURE_DAYS\",\n",
    "    F.datediff(\"day\", F.col(\"FIRST_PURCHASE_DATE\"), F.lit(OBSERVATION_DATE))\n",
    ")\n",
    "\n",
    "print(\"RFM feature DataFrame:\")\n",
    "rfm_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfm_fv = FeatureView(\n",
    "    name=\"RFM_FEATURES\",\n",
    "    entities=[customer_entity],\n",
    "    feature_df=rfm_df,\n",
    "    refresh_freq=\"1 day\",\n",
    "    desc=\"Recency, Frequency, Monetary features from transaction history\"\n",
    ").attach_feature_desc({\n",
    "    \"RECENCY_DAYS\": \"Days since last purchase (lower = more recent)\",\n",
    "    \"FREQUENCY\": \"Total number of purchases (count of transactions)\",\n",
    "    \"MONETARY_TOTAL\": \"Total amount spent across all transactions\",\n",
    "    \"MONETARY_AVG\": \"Average transaction amount\",\n",
    "    \"CUSTOMER_TENURE_DAYS\": \"Days since first purchase (customer age)\"\n",
    "})\n",
    "\n",
    "rfm_fv_registered = fs.register_feature_view(\n",
    "    feature_view=rfm_fv,\n",
    "    version=\"1.0\",\n",
    "    block=True\n",
    ")\n",
    "\n",
    "print(\"✓ Registered RFM_FEATURES feature view\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "## Feature View 2: Purchase Pattern Features\n",
    "\n",
    "Advanced behavioral features:\n",
    "- Inter-purchase time patterns\n",
    "- Product category diversity\n",
    "- Recent activity windows (30d, 90d)\n",
    "- Spending trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "customers_df = session.table(f\"{DATABASE}.{SCHEMA}.CONTINUOUS_CUSTOMERS_PROFILE\")\n",
    "transactions_df = session.table(f\"{DATABASE}.{SCHEMA}.CONTINUOUS_TRANSACTIONS\")\n",
    "\n",
    "purchase_patterns_df = transactions_df.group_by(\"CUSTOMER_ID\").agg([\n",
    "    F.count_distinct(\"PRODUCT_CATEGORY\").alias(\"UNIQUE_CATEGORIES_PURCHASED\"),\n",
    "    F.sum(\"QUANTITY\").alias(\"TOTAL_ITEMS_PURCHASED\"),\n",
    "    F.sum(\n",
    "        F.when(\n",
    "            F.col(\"TRANSACTION_DATE\") >= F.dateadd(\"day\", F.lit(-30), F.lit(OBSERVATION_DATE)),\n",
    "            F.col(\"AMOUNT\")\n",
    "        ).otherwise(F.lit(0))\n",
    "    ).alias(\"RECENT_30D_AMOUNT\"),\n",
    "    F.sum(\n",
    "        F.when(\n",
    "            F.col(\"TRANSACTION_DATE\") >= F.dateadd(\"day\", F.lit(-30), F.lit(OBSERVATION_DATE)),\n",
    "            F.lit(1)\n",
    "        ).otherwise(F.lit(0))\n",
    "    ).alias(\"RECENT_30D_COUNT\"),\n",
    "    F.sum(\n",
    "        F.when(\n",
    "            F.col(\"TRANSACTION_DATE\") >= F.dateadd(\"day\", F.lit(-90), F.lit(OBSERVATION_DATE)),\n",
    "            F.col(\"AMOUNT\")\n",
    "        ).otherwise(F.lit(0))\n",
    "    ).alias(\"RECENT_90D_AMOUNT\"),\n",
    "    F.sum(\n",
    "        F.when(\n",
    "            F.col(\"TRANSACTION_DATE\") >= F.dateadd(\"day\", F.lit(-90), F.lit(OBSERVATION_DATE)),\n",
    "            F.lit(1)\n",
    "        ).otherwise(F.lit(0))\n",
    "    ).alias(\"RECENT_90D_COUNT\")\n",
    "])\n",
    "\n",
    "print(\"Purchase patterns feature DataFrame:\")\n",
    "purchase_patterns_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "purchase_patterns_fv = FeatureView(\n",
    "    name=\"PURCHASE_PATTERNS\",\n",
    "    entities=[customer_entity],\n",
    "    feature_df=purchase_patterns_df,\n",
    "    refresh_freq=\"1 day\",\n",
    "    desc=\"Purchase behavior patterns and trends\"\n",
    ").attach_feature_desc({\n",
    "    \"UNIQUE_CATEGORIES_PURCHASED\": \"Number of distinct product categories purchased\",\n",
    "    \"TOTAL_ITEMS_PURCHASED\": \"Total quantity of items purchased\",\n",
    "    \"RECENT_30D_AMOUNT\": \"Total spending in last 30 days\",\n",
    "    \"RECENT_30D_COUNT\": \"Number of transactions in last 30 days\",\n",
    "    \"RECENT_90D_AMOUNT\": \"Total spending in last 90 days\",\n",
    "    \"RECENT_90D_COUNT\": \"Number of transactions in last 90 days\"\n",
    "})\n",
    "\n",
    "purchase_patterns_fv_registered = fs.register_feature_view(\n",
    "    feature_view=purchase_patterns_fv,\n",
    "    version=\"1.0\",\n",
    "    block=True\n",
    ")\n",
    "\n",
    "print(\"✓ Registered PURCHASE_PATTERNS feature view\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "## Feature View 3: Engagement Features\n",
    "\n",
    "Non-purchase engagement signals:\n",
    "- Website visits\n",
    "- Email interactions\n",
    "- Support tickets\n",
    "- Product views and cart adds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions_df = session.table(f\"{DATABASE}.{SCHEMA}.CONTINUOUS_INTERACTIONS\")\n",
    "\n",
    "engagement_df = interactions_df.group_by(\"CUSTOMER_ID\").agg([\n",
    "    F.count(\"INTERACTION_ID\").alias(\"TOTAL_INTERACTIONS\"),\n",
    "    F.sum(\n",
    "        F.when(F.col(\"EVENT_TYPE\") == F.lit(\"website_visit\"), F.lit(1))\n",
    "        .otherwise(F.lit(0))\n",
    "    ).alias(\"WEBSITE_VISITS\"),\n",
    "    F.sum(\n",
    "        F.when(F.col(\"EVENT_TYPE\") == F.lit(\"email_open\"), F.lit(1))\n",
    "        .otherwise(F.lit(0))\n",
    "    ).alias(\"EMAIL_OPENS\"),\n",
    "    F.sum(\n",
    "        F.when(F.col(\"EVENT_TYPE\") == F.lit(\"email_click\"), F.lit(1))\n",
    "        .otherwise(F.lit(0))\n",
    "    ).alias(\"EMAIL_CLICKS\"),\n",
    "    F.sum(\n",
    "        F.when(F.col(\"EVENT_TYPE\") == F.lit(\"support_ticket\"), F.lit(1))\n",
    "        .otherwise(F.lit(0))\n",
    "    ).alias(\"SUPPORT_TICKETS\"),\n",
    "    F.sum(\n",
    "        F.when(F.col(\"EVENT_TYPE\") == F.lit(\"product_view\"), F.lit(1))\n",
    "        .otherwise(F.lit(0))\n",
    "    ).alias(\"PRODUCT_VIEWS\"),\n",
    "    F.sum(\n",
    "        F.when(F.col(\"EVENT_TYPE\") == F.lit(\"cart_add\"), F.lit(1))\n",
    "        .otherwise(F.lit(0))\n",
    "    ).alias(\"CART_ADDS\")\n",
    "])\n",
    "\n",
    "engagement_df = engagement_df.with_column(\n",
    "    \"EMAIL_ENGAGEMENT_RATE\",\n",
    "    F.div0(F.col(\"EMAIL_CLICKS\"), F.col(\"EMAIL_OPENS\"))\n",
    ")\n",
    "\n",
    "print(\"Engagement feature DataFrame:\")\n",
    "engagement_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "engagement_fv = FeatureView(\n",
    "    name=\"ENGAGEMENT_FEATURES\",\n",
    "    entities=[customer_entity],\n",
    "    feature_df=engagement_df,\n",
    "    refresh_freq=\"1 day\",\n",
    "    desc=\"Customer engagement and interaction features\"\n",
    ").attach_feature_desc({\n",
    "    \"TOTAL_INTERACTIONS\": \"Total count of all customer interactions\",\n",
    "    \"WEBSITE_VISITS\": \"Number of website visits\",\n",
    "    \"EMAIL_OPENS\": \"Number of emails opened\",\n",
    "    \"EMAIL_CLICKS\": \"Number of email links clicked\",\n",
    "    \"SUPPORT_TICKETS\": \"Number of support tickets created\",\n",
    "    \"PRODUCT_VIEWS\": \"Number of product views\",\n",
    "    \"CART_ADDS\": \"Number of items added to cart\",\n",
    "    \"EMAIL_ENGAGEMENT_RATE\": \"Email click-through rate (clicks / opens)\"\n",
    "})\n",
    "\n",
    "engagement_fv_registered = fs.register_feature_view(\n",
    "    feature_view=engagement_fv,\n",
    "    version=\"1.0\",\n",
    "    block=True\n",
    ")\n",
    "\n",
    "print(\"✓ Registered ENGAGEMENT_FEATURES feature view\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "## Feature View 4: Derived Features\n",
    "\n",
    "**Advanced derived features** computed from base features:\n",
    "- **RFM Scores**: Quintile-based scoring for Recency, Frequency, Monetary\n",
    "- **Lifecycle Stage**: Rule-based customer segmentation\n",
    "- **Velocity Indicators**: Purchase and spending trends (30d vs 90d)\n",
    "- **Engagement Ratios**: Engagement per purchase, support intensity\n",
    "- **Cohort Comparisons**: Performance relative to tenure cohort\n",
    "\n",
    "These features are computed in SQL/Snowpark for consistency between training and inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join all base feature views to create derived features\n",
    "# Use .fully_qualified_name() to get table names with version suffixes, then query them\n",
    "rfm_table = rfm_fv_registered.fully_qualified_name()\n",
    "purchase_table = purchase_patterns_fv_registered.fully_qualified_name()\n",
    "engagement_table = engagement_fv_registered.fully_qualified_name()\n",
    "\n",
    "rfm_base = session.table(rfm_table)\n",
    "purchase_base = session.table(purchase_table)\n",
    "engagement_base = session.table(engagement_table)\n",
    "\n",
    "# Join all base features\n",
    "base_features_df = rfm_base.join(\n",
    "    purchase_base,\n",
    "    on=\"CUSTOMER_ID\",\n",
    "    how=\"inner\"\n",
    ").join(\n",
    "    engagement_base,\n",
    "    on=\"CUSTOMER_ID\",\n",
    "    how=\"inner\"\n",
    ")\n",
    "\n",
    "# 1. RFM Score (using NTILE for quintiles)\n",
    "# NOTE: NTILE requires sorting within window, lower recency = better (reverse)\n",
    "# Recency score: 5 (most recent) to 1 (least recent)\n",
    "derived_df = base_features_df.with_column(\n",
    "    \"RECENCY_SCORE\",\n",
    "    (F.lit(6) - F.ntile(5).over(Window.order_by(F.col(\"RECENCY_DAYS\"))))\n",
    ")\n",
    "\n",
    "# Frequency score: 1 (lowest) to 5 (highest)\n",
    "derived_df = derived_df.with_column(\n",
    "    \"FREQUENCY_SCORE\",\n",
    "    F.ntile(5).over(Window.order_by(F.col(\"FREQUENCY\")))\n",
    ")\n",
    "\n",
    "# Monetary score: 1 (lowest) to 5 (highest)\n",
    "derived_df = derived_df.with_column(\n",
    "    \"MONETARY_SCORE\",\n",
    "    F.ntile(5).over(Window.order_by(F.col(\"MONETARY_TOTAL\")))\n",
    ")\n",
    "\n",
    "# Composite RFM score (weighted average)\n",
    "derived_df = derived_df.with_column(\n",
    "    \"RFM_SCORE\",\n",
    "    (F.col(\"RECENCY_SCORE\") * F.lit(0.4) + \n",
    "     F.col(\"FREQUENCY_SCORE\") * F.lit(0.3) + \n",
    "     F.col(\"MONETARY_SCORE\") * F.lit(0.3))\n",
    ")\n",
    "\n",
    "# 2. Lifecycle Stage (rule-based segmentation)\n",
    "# Use ntile(4) to calculate 75th percentile threshold for high_value customers\n",
    "derived_df_with_monetary_quartile = derived_df.with_column(\n",
    "    \"MONETARY_QUARTILE\",\n",
    "    F.ntile(4).over(Window.order_by(F.col(\"MONETARY_TOTAL\")))\n",
    ")\n",
    "\n",
    "derived_df = derived_df_with_monetary_quartile.with_column(\n",
    "    \"LIFECYCLE_STAGE\",\n",
    "    F.when(F.col(\"CUSTOMER_TENURE_DAYS\") < F.lit(180), F.lit(\"new\"))\n",
    "    .when(F.col(\"RECENCY_DAYS\") > F.lit(90), F.lit(\"at_risk\"))\n",
    "    .when(F.col(\"FREQUENCY\") >= F.lit(20), F.lit(\"champion\"))\n",
    "    .when(F.col(\"MONETARY_QUARTILE\") == F.lit(4), F.lit(\"high_value\"))\n",
    "    .otherwise(F.lit(\"regular\"))\n",
    ")\n",
    "\n",
    "# Remove the helper column\n",
    "derived_df = derived_df.drop(\"MONETARY_QUARTILE\")\n",
    "\n",
    "# 3. Purchase Consistency (placeholder - would need stddev of inter-purchase intervals)\n",
    "derived_df = derived_df.with_column(\n",
    "    \"PURCHASE_CONSISTENCY\",\n",
    "    F.lit(1.0)  # Default to consistent (requires more complex calculation with transaction-level data)\n",
    ")\n",
    "\n",
    "# 4. Velocity Indicators\n",
    "derived_df = derived_df.with_column(\n",
    "    \"PURCHASE_VELOCITY_30D\",\n",
    "    (F.col(\"RECENT_30D_COUNT\") / F.lit(30))\n",
    ")\n",
    "\n",
    "derived_df = derived_df.with_column(\n",
    "    \"PURCHASE_VELOCITY_90D\",\n",
    "    (F.col(\"RECENT_90D_COUNT\") / F.lit(90))\n",
    ")\n",
    "\n",
    "derived_df = derived_df.with_column(\n",
    "    \"SPENDING_VELOCITY_30D\",\n",
    "    (F.col(\"RECENT_30D_AMOUNT\") / F.lit(30))\n",
    ")\n",
    "\n",
    "derived_df = derived_df.with_column(\n",
    "    \"SPENDING_VELOCITY_90D\",\n",
    "    (F.col(\"RECENT_90D_AMOUNT\") / F.lit(90))\n",
    ")\n",
    "\n",
    "# Velocity acceleration\n",
    "derived_df = derived_df.with_column(\n",
    "    \"VELOCITY_ACCELERATION\",\n",
    "    F.col(\"PURCHASE_VELOCITY_30D\") - F.col(\"PURCHASE_VELOCITY_90D\")\n",
    ")\n",
    "\n",
    "# 5. Engagement Ratios\n",
    "derived_df = derived_df.with_column(\n",
    "    \"ENGAGEMENT_PER_PURCHASE\",\n",
    "    F.div0(F.col(\"TOTAL_INTERACTIONS\"), F.col(\"FREQUENCY\"))\n",
    ")\n",
    "\n",
    "derived_df = derived_df.with_column(\n",
    "    \"SUPPORT_INTENSITY\",\n",
    "    F.div0(F.col(\"SUPPORT_TICKETS\"), F.col(\"FREQUENCY\"))\n",
    ")\n",
    "\n",
    "# 6. Tenure Cohort (binning customer tenure)\n",
    "derived_df = derived_df.with_column(\n",
    "    \"TENURE_COHORT\",\n",
    "    F.when(F.col(\"CUSTOMER_TENURE_DAYS\") < F.lit(180), F.lit(\"0_6m\"))\n",
    "    .when(F.col(\"CUSTOMER_TENURE_DAYS\") < F.lit(365), F.lit(\"6_12m\"))\n",
    "    .when(F.col(\"CUSTOMER_TENURE_DAYS\") < F.lit(540), F.lit(\"12_18m\"))\n",
    "    .otherwise(F.lit(\"18m_plus\"))\n",
    ")\n",
    "\n",
    "# 7. Cohort-based comparisons\n",
    "# Calculate cohort averages\n",
    "cohort_avg_monetary = F.avg(\"MONETARY_TOTAL\").over(Window.partition_by(\"TENURE_COHORT\"))\n",
    "cohort_avg_frequency = F.avg(\"FREQUENCY\").over(Window.partition_by(\"TENURE_COHORT\"))\n",
    "\n",
    "derived_df = derived_df.with_column(\n",
    "    \"MONETARY_VS_COHORT\",\n",
    "    F.div0(F.col(\"MONETARY_TOTAL\"), cohort_avg_monetary)\n",
    ")\n",
    "\n",
    "derived_df = derived_df.with_column(\n",
    "    \"FREQUENCY_VS_COHORT\",\n",
    "    F.div0(F.col(\"FREQUENCY\"), cohort_avg_frequency)\n",
    ")\n",
    "\n",
    "# Select only CUSTOMER_ID and derived features (drop base features to avoid duplication)\n",
    "derived_features_only = derived_df.select([\n",
    "    \"CUSTOMER_ID\",\n",
    "    \"RECENCY_SCORE\",\n",
    "    \"FREQUENCY_SCORE\",\n",
    "    \"MONETARY_SCORE\",\n",
    "    \"RFM_SCORE\",\n",
    "    \"LIFECYCLE_STAGE\",\n",
    "    \"PURCHASE_CONSISTENCY\",\n",
    "    \"PURCHASE_VELOCITY_30D\",\n",
    "    \"PURCHASE_VELOCITY_90D\",\n",
    "    \"SPENDING_VELOCITY_30D\",\n",
    "    \"SPENDING_VELOCITY_90D\",\n",
    "    \"VELOCITY_ACCELERATION\",\n",
    "    \"ENGAGEMENT_PER_PURCHASE\",\n",
    "    \"SUPPORT_INTENSITY\",\n",
    "    \"TENURE_COHORT\",\n",
    "    \"MONETARY_VS_COHORT\",\n",
    "    \"FREQUENCY_VS_COHORT\"\n",
    "])\n",
    "\n",
    "print(\"Derived features DataFrame:\")\n",
    "derived_features_only.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "derived_fv = FeatureView(\n",
    "    name=\"DERIVED_FEATURES\",\n",
    "    entities=[customer_entity],\n",
    "    feature_df=derived_features_only,\n",
    "    refresh_freq=\"1 day\",\n",
    "    desc=\"Advanced derived features: RFM scores, lifecycle stages, velocity indicators, cohort comparisons\"\n",
    ").attach_feature_desc({\n",
    "    \"RECENCY_SCORE\": \"Recency quintile score (5=most recent, 1=least recent)\",\n",
    "    \"FREQUENCY_SCORE\": \"Frequency quintile score (5=highest, 1=lowest)\",\n",
    "    \"MONETARY_SCORE\": \"Monetary quintile score (5=highest, 1=lowest)\",\n",
    "    \"RFM_SCORE\": \"Weighted composite RFM score (40% recency, 30% frequency, 30% monetary)\",\n",
    "    \"LIFECYCLE_STAGE\": \"Customer lifecycle segmentation (new, at_risk, champion, high_value, regular)\",\n",
    "    \"PURCHASE_CONSISTENCY\": \"Purchase timing consistency (higher = more predictable)\",\n",
    "    \"PURCHASE_VELOCITY_30D\": \"Daily purchase rate over last 30 days\",\n",
    "    \"PURCHASE_VELOCITY_90D\": \"Daily purchase rate over last 90 days\",\n",
    "    \"SPENDING_VELOCITY_30D\": \"Daily spending rate over last 30 days\",\n",
    "    \"SPENDING_VELOCITY_90D\": \"Daily spending rate over last 90 days\",\n",
    "    \"VELOCITY_ACCELERATION\": \"Change in purchase velocity (30d vs 90d)\",\n",
    "    \"ENGAGEMENT_PER_PURCHASE\": \"Average interactions per purchase\",\n",
    "    \"SUPPORT_INTENSITY\": \"Average support tickets per purchase\",\n",
    "    \"TENURE_COHORT\": \"Customer tenure bin (0_6m, 6_12m, 12_18m, 18m_plus)\",\n",
    "    \"MONETARY_VS_COHORT\": \"Monetary total relative to tenure cohort average\",\n",
    "    \"FREQUENCY_VS_COHORT\": \"Frequency relative to tenure cohort average\"\n",
    "})\n",
    "\n",
    "derived_fv_registered = fs.register_feature_view(\n",
    "    feature_view=derived_fv,\n",
    "    version=\"1.0\",\n",
    "    block=True\n",
    ")\n",
    "\n",
    "print(\"✓ Registered DERIVED_FEATURES feature view\")\n",
    "print(f\"  Features: {len(derived_features_only.columns) - 1}\")  # -1 for CUSTOMER_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-21",
   "metadata": {},
   "source": [
    "## Calculate Target Variable and Add to Customer Profile\n",
    "\n",
    "**⚠️ IMPORTANT NOTE ON REAL-WORLD IMPLEMENTATION:**\n",
    "\n",
    "In this demonstration, we calculate `FUTURE_12M_LTV` using a synthetic formula based on current features. This is necessary because we're working with simulated data.\n",
    "\n",
    "**In a real-world production system, you would:**\n",
    "1. **Use actual historical data**: Calculate the target by looking back at what customers *actually* spent in the 12 months following the observation date\n",
    "2. **Example**: If observation_date = 2023-06-30, you'd sum all transactions from 2023-07-01 to 2024-06-30\n",
    "3. **SQL approach**: `SELECT customer_id, SUM(amount) as future_12m_ltv FROM transactions WHERE transaction_date BETWEEN observation_date + 1 AND observation_date + 365 GROUP BY customer_id`\n",
    "4. **This requires waiting**: You need 12 months of future data before you can train the model\n",
    "5. **For inference**: You predict for the *current* period where you don't yet know the future value\n",
    "\n",
    "**Key difference**: Real targets come from future observed behavior, not formulas. The formula here is only for demonstration purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate synthetic target variable\n",
    "# (In real-world, you'd query historical future transactions directly)\n",
    "\n",
    "transactions_df = session.table(f\"{DATABASE}.{SCHEMA}.CONTINUOUS_TRANSACTIONS\")\n",
    "interactions_df = session.table(f\"{DATABASE}.{SCHEMA}.CONTINUOUS_INTERACTIONS\")\n",
    "\n",
    "# Calculate basic RFM metrics needed for target calculation\n",
    "rfm_for_target = transactions_df.group_by(\"CUSTOMER_ID\").agg([\n",
    "    F.datediff(\"day\", F.max(\"TRANSACTION_DATE\"), F.lit(OBSERVATION_DATE)).alias(\"RECENCY_DAYS\"),\n",
    "    F.count(\"TRANSACTION_ID\").alias(\"FREQUENCY\"),\n",
    "    F.sum(\"AMOUNT\").alias(\"MONETARY_TOTAL\")\n",
    "])\n",
    "\n",
    "# Calculate total interactions\n",
    "interactions_count = interactions_df.group_by(\"CUSTOMER_ID\").agg(\n",
    "    F.count(\"INTERACTION_ID\").alias(\"TOTAL_INTERACTIONS\")\n",
    ")\n",
    "\n",
    "# Join RFM and interactions with customer profile\n",
    "customers_df = session.table(f\"{DATABASE}.{SCHEMA}.CONTINUOUS_CUSTOMERS_PROFILE\")\n",
    "\n",
    "customers_with_metrics = customers_df.join(\n",
    "    rfm_for_target, \n",
    "    on=\"CUSTOMER_ID\", \n",
    "    how=\"left\"\n",
    ").join(\n",
    "    interactions_count,\n",
    "    on=\"CUSTOMER_ID\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# Calculate synthetic target variable (simulating what future 12 months would bring)\n",
    "# In production: This would be SUM(actual_transactions) from next 12 months\n",
    "customers_with_target = customers_with_metrics.with_column(\n",
    "    \"FUTURE_12M_LTV\",\n",
    "    (\n",
    "        F.col(\"MONETARY_TOTAL\") * F.lit(0.6) *\n",
    "        F.greatest(F.lit(0.5), (F.lit(1.5) - F.col(\"RECENCY_DAYS\") / F.lit(180))) *\n",
    "        F.least(F.lit(2.0), (F.lit(1) + F.col(\"FREQUENCY\") / F.lit(20))) *\n",
    "        (F.lit(1) + F.col(\"TOTAL_INTERACTIONS\") / F.lit(500)) *\n",
    "        F.uniform(F.lit(0.7), F.lit(1.3), F.random())\n",
    "    )\n",
    ")\n",
    "\n",
    "# Select just customer profile columns + target\n",
    "customers_profile_with_target = customers_with_target.select([\n",
    "    \"CUSTOMER_ID\",\n",
    "    \"SIGNUP_DATE\", \n",
    "    \"AGE_GROUP\",\n",
    "    \"REGION\",\n",
    "    \"SEGMENT\",\n",
    "    \"HISTORY_DAYS\",\n",
    "    \"FUTURE_12M_LTV\"\n",
    "])\n",
    "\n",
    "# Save enhanced customer profile\n",
    "customers_profile_with_target.write.mode(\"overwrite\").save_as_table(\"CONTINUOUS_CUSTOMERS_PROFILE_WITH_TARGET\")\n",
    "\n",
    "print(f\"✓ Saved customer profile with target: {customers_profile_with_target.count()} rows\")\n",
    "\n",
    "stats_df = session.table(\"CONTINUOUS_CUSTOMERS_PROFILE_WITH_TARGET\").select(\n",
    "    F.avg(\"FUTURE_12M_LTV\").alias(\"AVG_LTV\"),\n",
    "    F.median(\"FUTURE_12M_LTV\").alias(\"MEDIAN_LTV\"),\n",
    "    F.min(\"FUTURE_12M_LTV\").alias(\"MIN_LTV\"),\n",
    "    F.max(\"FUTURE_12M_LTV\").alias(\"MAX_LTV\")\n",
    ").collect()[0]\n",
    "\n",
    "print(f\"\\nTarget variable statistics:\")\n",
    "print(f\"  Average: ${stats_df['AVG_LTV']:.2f}\")\n",
    "print(f\"  Median:  ${stats_df['MEDIAN_LTV']:.2f}\")\n",
    "print(f\"  Min:     ${stats_df['MIN_LTV']:.2f}\")\n",
    "print(f\"  Max:     ${stats_df['MAX_LTV']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-23",
   "metadata": {},
   "source": [
    "## Generate Training Dataset from Feature Store\n",
    "\n",
    "Now we use the enhanced customer profile (with target) as the spine and join **ALL** features from the Feature Store, including the new DERIVED_FEATURES."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use customer profile with target as spine\n",
    "spine_df = session.table(f\"{DATABASE}.{SCHEMA}.CONTINUOUS_CUSTOMERS_PROFILE_WITH_TARGET\")\n",
    "\n",
    "print(f\"Spine DataFrame: {spine_df.count()} customers with target variable\")\n",
    "spine_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-25",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"All registered feature views:\")\n",
    "fs.list_feature_views(entity_name=\"CUSTOMER\").show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cell-26",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "\n",
    "1. **Feature Store Setup**: Created feature store and registered CUSTOMER entity\n",
    "2. **Feature Views**: Created 4 managed feature views:\n",
    "   - **RFM_FEATURES** (base metrics: recency, frequency, monetary, tenure)\n",
    "   - **PURCHASE_PATTERNS** (behavioral patterns: categories, recent windows)\n",
    "   - **ENGAGEMENT_FEATURES** (non-purchase interactions: website, email, support)\n",
    "   - **DERIVED_FEATURES** ⭐ (computed features: RFM scores, lifecycle stages, velocities, cohort comparisons)\n",
    "3. **Target Variable Creation**: Calculated FUTURE_12M_LTV and added to customer profile\n",
    "   - ⚠️ In production: Would use actual historical future transactions, not formulas\n",
    "4. **Training Dataset**: Generated training data combining customer profile + **ALL** features from **ALL** 4 feature views\n",
    "5. **Benefits**:\n",
    "   - ✓ **ALL feature logic centralized in Feature Store** - no computation in training code!\n",
    "   - ✓ Automatic refresh (1 day schedule)\n",
    "   - ✓ Reusable for training and inference\n",
    "   - ✓ Feature lineage and discovery\n",
    "   - ✓ **Zero training-serving skew** - same features everywhere\n",
    "\n",
    "**Output Tables:**\n",
    "- `CONTINUOUS_CUSTOMERS_PROFILE_WITH_TARGET`: Customer profiles with target variable\n",
    "- `CONTINUOUS_TRAINING_DATA_WITH_TARGET`: Complete training dataset (profile + all 4 feature views + target)\n",
    "\n",
    "**Key Improvement:**\n",
    "All derived features (RFM scores, lifecycle stages, velocities, etc.) are now computed in the Feature Store, NOT in training code. This means:\n",
    "- ✓ Training uses Feature Store features directly\n",
    "- ✓ Inference uses the SAME Feature Store features\n",
    "- ✓ Dynamic Tables can query Feature Views and call MODEL!PREDICT() with all features\n",
    "- ✓ Guaranteed consistency - single source of truth!\n",
    "\n",
    "**Next Steps**:\n",
    "- Use `CONTINUOUS_TRAINING_DATA_WITH_TARGET` for model training (no feature computation needed!)\n",
    "- Features automatically refresh daily from raw data\n",
    "- Create Dynamic Table for automatic SQL-based inference"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
