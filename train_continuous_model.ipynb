{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Continuous CLV Prediction Model\n",
    "\n",
    "This notebook trains an XGBoost model to predict 12-month customer lifetime value for established customers.\n",
    "\n",
    "**Data Source**: Features from Snowflake Feature Store (feature_engineering_continuous.ipynb)\n",
    "\n",
    "**Model Purpose**: Provide updated CLV predictions for customers with 3+ months of history, enabling dynamic segmentation and retention strategies.\n",
    "\n",
    "**Steps**:\n",
    "1. Load training data from Feature Store\n",
    "2. Create additional behavioral features\n",
    "3. Train XGBoost with hyperparameter tuning\n",
    "4. Evaluate model performance\n",
    "5. Deploy to Snowflake Model Registry with Feature Store lineage\n",
    "\n"
   ],
   "id": "cell-0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import xgboost as xgb\n",
    "from snowflake.snowpark.context import get_active_session\n",
    "from snowflake.ml.registry import Registry\n",
    "from snowflake.ml.model import task\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Get active Snowflake session\n",
    "session = get_active_session()"
   ],
   "id": "cell-1"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Set your database and schema here:"
   ],
   "id": "cell-2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Database and schema configuration\n",
    "DATABASE = 'ML_DEMO'\n",
    "SCHEMA = 'PUBLIC'\n",
    "\n",
    "# Set context\n",
    "session.use_database(DATABASE)\n",
    "session.use_schema(SCHEMA)\n",
    "\n",
    "print(f\"Using database: {DATABASE}\")\n",
    "print(f\"Using schema: {SCHEMA}\")\n",
    "print(f\"Current warehouse: {session.get_current_warehouse()}\")\n",
    "print(f\"Current role: {session.get_current_role()}\")"
   ],
   "id": "cell-3"
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## Load Data from Snowflake"
   ],
   "attachments": {}
  },
  {
   "cell_type": "code",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "# Read training data from Feature Store output table\n",
    "table_name = 'CONTINUOUS_TRAINING_DATA_WITH_TARGET'\n",
    "df = session.table(table_name).to_pandas()\n",
    "\n",
    "# Convert date columns if present\n",
    "date_columns = ['SIGNUP_DATE', 'FIRST_PURCHASE_DATE', 'LAST_PURCHASE_DATE']\n",
    "for col in date_columns:\n",
    "    if col in df.columns:\n",
    "        df[col] = pd.to_datetime(df[col])\n",
    "\n",
    "print(f\"Loaded {len(df)} customer records from {DATABASE}.{SCHEMA}.{table_name}\")\n",
    "print(f\"Features: {len(df.columns)} columns\")\n",
    "print(f\"\\nData comes from Feature Store with automatic feature lineage\")\n",
    "df.head()\n",
    "\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Feature Engineering\n",
    "\n",
    "### RFM Score Normalization\n",
    "**Rationale**: RFM are the foundation of CLV prediction. Normalizing them helps the model compare across different scales."
   ],
   "id": "cell-6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize column names to lowercase for processing\n",
    "df.columns = df.columns.str.lower()\n",
    "\n",
    "# RFM Score Normalization\n",
    "df['recency_score'] = pd.qcut(df['recency_days'], q=5, labels=[5, 4, 3, 2, 1], duplicates='drop')\n",
    "df['frequency_score'] = pd.qcut(df['frequency'], q=5, labels=[1, 2, 3, 4, 5], duplicates='drop')\n",
    "df['monetary_score'] = pd.qcut(df['monetary_total'], q=5, labels=[1, 2, 3, 4, 5], duplicates='drop')\n",
    "\n",
    "df['rfm_score'] = (\n",
    "    df['recency_score'].astype(float) * 0.4 + \n",
    "    df['frequency_score'].astype(float) * 0.3 + \n",
    "    df['monetary_score'].astype(float) * 0.3\n",
    ")\n",
    "\n",
    "print(f\"RFM Score range: {df['rfm_score'].min():.2f} to {df['rfm_score'].max():.2f}\")\n",
    "\n"
   ],
   "id": "cell-7"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Customer Lifecycle Stage\n",
    "**Rationale**: Customer maturity affects future value. New customers may have growth potential, while long-tenure customers show stability."
   ],
   "id": "cell-8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_lifecycle_stage(row):\n",
    "    if row['customer_tenure_days'] < 180:\n",
    "        return 'new'\n",
    "    elif row['recency_days'] > 90:\n",
    "        return 'at_risk'\n",
    "    elif row['frequency'] >= 20:\n",
    "        return 'champion'\n",
    "    elif row['monetary_total'] >= df['monetary_total'].quantile(0.75):\n",
    "        return 'high_value'\n",
    "    else:\n",
    "        return 'regular'\n",
    "\n",
    "df['lifecycle_stage'] = df.apply(assign_lifecycle_stage, axis=1)\n",
    "\n",
    "print(\"\\nLifecycle stage distribution:\")\n",
    "print(df['lifecycle_stage'].value_counts())"
   ],
   "id": "cell-9"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Purchase Consistency Score\n",
    "**Rationale**: Customers with consistent purchase intervals are more predictable and likely to continue."
   ],
   "id": "cell-10"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing values in inter-purchase days standard deviation\n",
    "df['std_inter_purchase_days'] = 0  # Feature Store doesn't compute this yet\n",
    "df['purchase_consistency'] = 1.0  # Default to consistent\n",
    "\n",
    "# If we had inter-purchase std dev, we'd calculate:\n",
    "# df['purchase_consistency'] = 1 / (1 + df['std_inter_purchase_days'].fillna(0))\n",
    "\n",
    "df['purchase_consistency'].fillna(1.0, inplace=True)\n",
    "\n"
   ],
   "id": "cell-11"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Velocity Indicators\n",
    "**Rationale**: Recent activity trends (30/90 day windows) indicate momentum and current engagement level."
   ],
   "id": "cell-12"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use features from Feature Store (already computed)\n",
    "df['purchase_velocity_30d'] = df['recent_30d_count'] / 30\n",
    "df['purchase_velocity_90d'] = df['recent_90d_count'] / 90\n",
    "\n",
    "df['spending_velocity_30d'] = df['recent_30d_amount'] / 30\n",
    "df['spending_velocity_90d'] = df['recent_90d_amount'] / 90\n",
    "\n",
    "df['velocity_acceleration'] = df['purchase_velocity_30d'] - df['purchase_velocity_90d']\n",
    "\n",
    "print(\"✓ Calculated velocity indicators from Feature Store features\")\n",
    "\n"
   ],
   "id": "cell-13"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Engagement-to-Purchase Ratio\n",
    "**Rationale**: High engagement with low purchases may indicate barriers or opportunities for conversion improvement."
   ],
   "id": "cell-14"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['engagement_per_purchase'] = df['total_interactions'] / df['frequency'].replace(0, np.nan)\n",
    "df['engagement_per_purchase'].fillna(0, inplace=True)\n",
    "\n",
    "df['support_intensity'] = df['support_tickets'] / df['frequency'].replace(0, np.nan)\n",
    "df['support_intensity'].fillna(0, inplace=True)"
   ],
   "id": "cell-15"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cohort-Based Features\n",
    "**Rationale**: Comparing customer to their cohort (by tenure) provides context for performance evaluation."
   ],
   "id": "cell-16"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tenure_bins = [0, 180, 365, 540, 999999]\n",
    "tenure_labels = ['0-6m', '6-12m', '12-18m', '18m+']\n",
    "df['tenure_cohort'] = pd.cut(df['customer_tenure_days'], bins=tenure_bins, labels=tenure_labels)\n",
    "\n",
    "cohort_avg_monetary = df.groupby('tenure_cohort')['monetary_total'].transform('mean')\n",
    "df['monetary_vs_cohort'] = df['monetary_total'] / cohort_avg_monetary\n",
    "\n",
    "cohort_avg_frequency = df.groupby('tenure_cohort')['frequency'].transform('mean')\n",
    "df['frequency_vs_cohort'] = df['frequency'] / cohort_avg_frequency"
   ],
   "id": "cell-17"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Features and Target"
   ],
   "id": "cell-18"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = [\n",
    "    'age_group',\n",
    "    'region',\n",
    "    'segment',\n",
    "    'lifecycle_stage',\n",
    "    'tenure_cohort'\n",
    "]\n",
    "\n",
    "numerical_features = [\n",
    "    'recency_days',\n",
    "    'frequency',\n",
    "    'monetary_total',\n",
    "    'monetary_avg',\n",
    "    'customer_tenure_days',\n",
    "    'unique_categories_purchased',\n",
    "    'total_items_purchased',\n",
    "    'recent_30d_amount',\n",
    "    'recent_30d_count',\n",
    "    'recent_90d_amount',\n",
    "    'recent_90d_count',\n",
    "    'total_interactions',\n",
    "    'website_visits',\n",
    "    'email_opens',\n",
    "    'email_clicks',\n",
    "    'support_tickets',\n",
    "    'product_views',\n",
    "    'cart_adds',\n",
    "    'email_engagement_rate',\n",
    "    'rfm_score',\n",
    "    'purchase_consistency',\n",
    "    'purchase_velocity_30d',\n",
    "    'purchase_velocity_90d',\n",
    "    'spending_velocity_30d',\n",
    "    'spending_velocity_90d',\n",
    "    'velocity_acceleration',\n",
    "    'engagement_per_purchase',\n",
    "    'support_intensity',\n",
    "    'monetary_vs_cohort',\n",
    "    'frequency_vs_cohort'\n",
    "]\n",
    "\n",
    "# Fill missing values\n",
    "df[categorical_features] = df[categorical_features].fillna('unknown')\n",
    "df[numerical_features] = df[numerical_features].fillna(0)\n",
    "df[numerical_features] = df[numerical_features].replace([np.inf, -np.inf], 0)\n",
    "\n",
    "X = df[categorical_features + numerical_features]\n",
    "y = df['future_12m_ltv']\n",
    "\n",
    "print(f\"Feature matrix shape: {X.shape}\")\n",
    "print(f\"Target variable shape: {y.shape}\")\n",
    "print(f\"\\nFeatures from Feature Store: {len(categorical_features)} categorical, {len(numerical_features)} numerical\")\n",
    "print(\"✓ All features have lineage tracked in Feature Store\")\n",
    "\n"
   ],
   "id": "cell-19"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-Validation-Test Split\n",
    "\n",
    "**Temporal split**: Using customer tenure and last purchase date to simulate production deployment."
   ],
   "id": "cell-20"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sorted = df.sort_values('last_purchase_date').reset_index(drop=True)\n",
    "\n",
    "train_size = int(0.7 * len(df_sorted))\n",
    "val_size = int(0.15 * len(df_sorted))\n",
    "\n",
    "train_df = df_sorted.iloc[:train_size]\n",
    "val_df = df_sorted.iloc[train_size:train_size + val_size]\n",
    "test_df = df_sorted.iloc[train_size + val_size:]\n",
    "\n",
    "X_train = train_df[categorical_features + numerical_features]\n",
    "y_train = train_df['future_12m_ltv']\n",
    "\n",
    "X_val = val_df[categorical_features + numerical_features]\n",
    "y_val = val_df['future_12m_ltv']\n",
    "\n",
    "X_test = test_df[categorical_features + numerical_features]\n",
    "y_test = test_df['future_12m_ltv']\n",
    "\n",
    "print(f\"Train set: {len(X_train)} samples (mean LTV: ${y_train.mean():.2f})\")\n",
    "print(f\"Validation set: {len(X_val)} samples (mean LTV: ${y_val.mean():.2f})\")\n",
    "print(f\"Test set: {len(X_test)} samples (mean LTV: ${y_test.mean():.2f})\")"
   ],
   "id": "cell-21"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Preprocessing Pipeline"
   ],
   "id": "cell-22"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "X_train_processed = preprocessor.fit_transform(X_train)\n",
    "X_val_processed = preprocessor.transform(X_val)\n",
    "X_test_processed = preprocessor.transform(X_test)\n",
    "\n",
    "print(f\"Processed feature dimensionality: {X_train_processed.shape[1]}\")"
   ],
   "id": "cell-23"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning with GridSearchCV\n",
    "\n",
    "**Overfitting prevention strategies**:\n",
    "- Constrained tree depth and complexity\n",
    "- Row and column subsampling\n",
    "- L1 and L2 regularization\n",
    "- Early stopping on validation set\n",
    "- 5-fold cross-validation"
   ],
   "id": "cell-24"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = xgb.XGBRegressor(\n",
    "    objective='reg:squarederror',\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "param_grid = {\n",
    "    'N_ESTIMATORS': [100, 200, 300],\n",
    "    'MAX_DEPTH': [4, 6, 8],\n",
    "    'LEARNING_RATE': [0.01, 0.05, 0.1],\n",
    "    'MIN_CHILD_WEIGHT': [3, 5, 7],\n",
    "    'SUBSAMPLE': [0.7, 0.8, 0.9],\n",
    "    'COLSAMPLE_BYTREE': [0.7, 0.8, 0.9],\n",
    "    'REG_ALPHA': [0, 0.1, 0.5],\n",
    "    'REG_LAMBDA': [1, 1.5, 2]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=base_model,\n",
    "    param_grid=param_grid,\n",
    "    cv=5,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    n_jobs=-1,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "print(\"Starting hyperparameter tuning (this may take several minutes)...\")\n",
    "grid_search.fit(\n",
    "    X_train_processed, \n",
    "    y_train,\n",
    "    eval_set=[(X_val_processed, y_val)],\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "print(f\"\\nBest parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best CV score (neg MSE): {grid_search.best_score_:.2f}\")\n",
    "\n",
    "best_model = grid_search.best_estimator_"
   ],
   "id": "cell-25"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "\n",
    "**Key metrics**:\n",
    "- **RMSE**: Average prediction error in dollars\n",
    "- **MAE**: Typical absolute error\n",
    "- **R²**: Proportion of variance explained (closer to 1 is better)"
   ],
   "id": "cell-26"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = best_model.predict(X_train_processed)\n",
    "y_val_pred = best_model.predict(X_val_processed)\n",
    "y_test_pred = best_model.predict(X_test_processed)\n",
    "\n",
    "def evaluate_model(y_true, y_pred, dataset_name):\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    \n",
    "    mape = np.mean(np.abs((y_true - y_pred) / y_true.replace(0, np.nan))) * 100\n",
    "    \n",
    "    print(f\"\\n{dataset_name} Metrics:\")\n",
    "    print(f\"  RMSE: ${rmse:.2f}\")\n",
    "    print(f\"  MAE: ${mae:.2f}\")\n",
    "    print(f\"  R²: {r2:.4f}\")\n",
    "    print(f\"  MAPE: {mape:.2f}%\")\n",
    "    \n",
    "    return {'RMSE': rmse, 'MAE': mae, 'r2': r2, 'MAPE': mape}\n",
    "\n",
    "train_metrics = evaluate_model(y_train, y_train_pred, \"Train\")\n",
    "val_metrics = evaluate_model(y_val, y_val_pred, \"Validation\")\n",
    "test_metrics = evaluate_model(y_test, y_test_pred, \"Test\")\n",
    "\n",
    "generalization_gap = train_metrics['r2'] - test_metrics['r2']\n",
    "print(f\"\\nGeneralization gap (Train R² - Test R²): {generalization_gap:.4f}\")\n",
    "\n",
    "if generalization_gap > 0.1:\n",
    "    print(\"⚠️ Warning: Significant overfitting detected\")\n",
    "elif generalization_gap > 0.05:\n",
    "    print(\"⚠️ Caution: Moderate overfitting detected\")\n",
    "else:\n",
    "    print(\"✓ Model shows excellent generalization\")"
   ],
   "id": "cell-27"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction Distribution Analysis"
   ],
   "id": "cell-28"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].scatter(y_test, y_test_pred, alpha=0.5)\n",
    "axes[0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "axes[0].set_xlabel('Actual Future LTV ($)')\n",
    "axes[0].set_ylabel('Predicted Future LTV ($)')\n",
    "axes[0].set_title('Actual vs Predicted CLV')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "residuals = y_test - y_test_pred\n",
    "axes[1].hist(residuals, bins=50, edgecolor='black')\n",
    "axes[1].set_xlabel('Prediction Error ($)')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].set_title('Distribution of Prediction Errors')\n",
    "axes[1].axvline(0, color='r', linestyle='--', lw=2)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('continuous_model_predictions.png')\n",
    "plt.show()"
   ],
   "id": "cell-29"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance Analysis"
   ],
   "id": "cell-30"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = (\n",
    "    numerical_features + \n",
    "    list(preprocessor.named_transformers_['cat'].get_feature_names_out(categorical_features))\n",
    ")\n",
    "\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'FEATURE': feature_names,\n",
    "    'IMPORTANCE': best_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\nTop 20 Most Important Features:\")\n",
    "print(feature_importance_df.head(20))\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.barplot(data=feature_importance_df.head(20), x='importance', y='feature')\n",
    "plt.title('Top 20 Feature Importances - Continuous CLV Model')\n",
    "plt.xlabel('Importance')\n",
    "plt.tight_layout()\n",
    "plt.savefig('continuous_feature_importance.png')\n",
    "plt.show()"
   ],
   "id": "cell-31"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy to Snowflake Model Registry\n",
    "\n",
    "**Deployment Strategy**:\n",
    "- Use Snowflake Model Registry for versioning and management\n",
    "- Register with sample input for schema inference\n",
    "- Include comprehensive metrics for tracking"
   ],
   "id": "cell-32"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Model Registry with active session\n",
    "registry = Registry(session=session)\n",
    "\n",
    "# Prepare sample input for schema inference\n",
    "sample_input = X_train.head(100)\n",
    "\n",
    "# Log model to registry\n",
    "model_version = registry.log_model(\n",
    "    model=full_pipeline,\n",
    "    model_name=\"CONTINUOUS_CLV_MODEL\",\n",
    "    version_name=\"V1\",\n",
    "    comment=\"Continuous customer lifetime value prediction model for established customers using XGBoost\",\n",
    "    metrics={\n",
    "        \"test_rmse\": float(test_metrics['rmse']),\n",
    "        \"test_mae\": float(test_metrics['mae']),\n",
    "        \"test_r2\": float(test_metrics['r2']),\n",
    "        \"test_mape\": float(test_metrics['mape']),\n",
    "        \"train_r2\": float(train_metrics['r2']),\n",
    "        \"generalization_gap\": float(generalization_gap)\n",
    "    },\n",
    "    sample_input_data=sample_input,\n",
    "    task=task.Task.TABULAR_REGRESSION\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ Model registered successfully!\")\n",
    "print(f\"  Database: {DATABASE}\")\n",
    "print(f\"  Schema: {SCHEMA}\")\n",
    "print(f\"  Model: CONTINUOUS_CLV_MODEL\")\n",
    "print(f\"  Version: V1\")"
   ],
   "id": "cell-33"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection_name = os.getenv('SNOWFLAKE_CONNECTION_NAME') or 'default'\n",
    "\n",
    "session = Session.builder.configs({'CONNECTION_NAME': connection_name}).create()\n",
    "\n",
    "print(f\"Connected to Snowflake as: {session.get_current_user()}\")\n",
    "print(f\"Current database: {session.get_current_database()}\")\n",
    "print(f\"Current schema: {session.get_current_schema()}\")"
   ],
   "id": "cell-34"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "registry = Registry(session=session)\n",
    "\n",
    "sample_input = X_train.head(100)\n",
    "\n",
    "model_version = registry.log_model(\n",
    "    model=full_pipeline,\n",
    "    model_name=\"CONTINUOUS_CLV_MODEL\",\n",
    "    version_name=\"V1\",\n",
    "    comment=\"Continuous customer lifetime value prediction model for established customers using XGBoost\",\n",
    "    metrics={\n",
    "        \"test_rmse\": float(test_metrics['rmse']),\n",
    "        \"test_mae\": float(test_metrics['mae']),\n",
    "        \"test_r2\": float(test_metrics['r2']),\n",
    "        \"test_mape\": float(test_metrics['mape']),\n",
    "        \"train_r2\": float(train_metrics['r2']),\n",
    "        \"generalization_gap\": float(generalization_gap)\n",
    "    },\n",
    "    sample_input_data=sample_input,\n",
    "    task=task.Task.TABULAR_REGRESSION,\n",
    "    conda_dependencies=[\"xgboost\", \"scikit-learn\", \"pandas\", \"numpy\"]\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ Model registered successfully!\")\n",
    "print(f\"  Model: CONTINUOUS_CLV_MODEL\")\n",
    "print(f\"  Version: V1\")"
   ],
   "id": "cell-35"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dynamic Tables for Continuous Inference\n",
    "\n",
    "**Architecture**:\n",
    "1. Staging tables for raw transaction/interaction data\n",
    "2. Feature aggregation dynamic table\n",
    "3. Prediction dynamic table that calls the model"
   ],
   "id": "cell-36"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_transactions_staging = \"\"\"\n",
    "CREATE OR REPLACE TABLE CONTINUOUS_TRANSACTIONS_STAGING (\n",
    "    transaction_id INT,\n",
    "    customer_id INT,\n",
    "    transaction_date TIMESTAMP,\n",
    "    amount FLOAT,\n",
    "    product_category VARCHAR,\n",
    "    quantity INT\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "create_interactions_staging = \"\"\"\n",
    "CREATE OR REPLACE TABLE CONTINUOUS_INTERACTIONS_STAGING (\n",
    "    interaction_id INT,\n",
    "    customer_id INT,\n",
    "    event_date TIMESTAMP,\n",
    "    event_type VARCHAR\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "session.sql(create_transactions_staging).collect()\n",
    "session.sql(create_interactions_staging).collect()\n",
    "\n",
    "print(\"✓ Staging tables created:\")\n",
    "print(\"  - CONTINUOUS_TRANSACTIONS_STAGING\")\n",
    "print(\"  - CONTINUOUS_INTERACTIONS_STAGING\")"
   ],
   "id": "cell-37"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_feature_aggregation_dt = \"\"\"\n",
    "CREATE OR REPLACE DYNAMIC TABLE CONTINUOUS_CUSTOMER_FEATURES\n",
    "    TARGET_LAG = '1 hour'\n",
    "    WAREHOUSE = COMPUTE_WH\n",
    "    REFRESH_MODE = INCREMENTAL\n",
    "AS\n",
    "WITH rfm_metrics AS (\n",
    "    SELECT \n",
    "        customer_id,\n",
    "        DATEDIFF('day', MAX(transaction_date), CURRENT_TIMESTAMP()) AS recency_days,\n",
    "        COUNT(*) AS frequency,\n",
    "        SUM(amount) AS monetary_total,\n",
    "        AVG(amount) AS monetary_avg\n",
    "    FROM CONTINUOUS_TRANSACTIONS_STAGING\n",
    "    GROUP BY customer_id\n",
    "),\n",
    "engagement_metrics AS (\n",
    "    SELECT\n",
    "        customer_id,\n",
    "        COUNT(*) AS total_interactions,\n",
    "        SUM(CASE WHEN event_type = 'website_visit' THEN 1 ELSE 0 END) AS website_visits,\n",
    "        SUM(CASE WHEN event_type = 'email_open' THEN 1 ELSE 0 END) AS email_opens,\n",
    "        SUM(CASE WHEN event_type = 'email_click' THEN 1 ELSE 0 END) AS email_clicks,\n",
    "        SUM(CASE WHEN event_type = 'support_ticket' THEN 1 ELSE 0 END) AS support_tickets\n",
    "    FROM CONTINUOUS_INTERACTIONS_STAGING\n",
    "    GROUP BY customer_id\n",
    ")\n",
    "SELECT \n",
    "    r.*,\n",
    "    e.total_interactions,\n",
    "    e.website_visits,\n",
    "    e.email_opens,\n",
    "    e.email_clicks,\n",
    "    e.support_tickets\n",
    "FROM rfm_metrics r\n",
    "LEFT JOIN engagement_metrics e ON r.customer_id = e.customer_id\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    session.sql(create_feature_aggregation_dt).collect()\n",
    "    print(\"✓ Feature aggregation dynamic table created: CONTINUOUS_CUSTOMER_FEATURES\")\n",
    "except Exception as e:\n",
    "    print(f\"Note: Dynamic table creation may require adjusting warehouse name\")\n",
    "    print(f\"Error: {str(e)}\")"
   ],
   "id": "cell-38"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_prediction_dt = \"\"\"\n",
    "CREATE OR REPLACE DYNAMIC TABLE CONTINUOUS_CLV_PREDICTIONS\n",
    "    TARGET_LAG = '1 hour'\n",
    "    WAREHOUSE = COMPUTE_WH\n",
    "    REFRESH_MODE = AUTO\n",
    "AS\n",
    "SELECT \n",
    "    customer_id,\n",
    "    recency_days,\n",
    "    frequency,\n",
    "    monetary_total,\n",
    "    CONTINUOUS_CLV_MODEL!PREDICT(\n",
    "        -- Pass all required features in correct order\n",
    "        recency_days,\n",
    "        frequency,\n",
    "        monetary_total,\n",
    "        monetary_avg,\n",
    "        total_interactions,\n",
    "        website_visits,\n",
    "        email_opens,\n",
    "        email_clicks,\n",
    "        support_tickets\n",
    "        -- Add other features as needed based on model signature\n",
    "    ) AS predicted_12m_ltv,\n",
    "    CURRENT_TIMESTAMP() AS prediction_timestamp\n",
    "FROM CONTINUOUS_CUSTOMER_FEATURES\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    session.sql(create_prediction_dt).collect()\n",
    "    print(\"✓ Prediction dynamic table created: CONTINUOUS_CLV_PREDICTIONS\")\n",
    "    print(\"  - Refreshes hourly\")\n",
    "    print(\"  - Incrementally processes new transactions\")\n",
    "    print(\"  - Automatically scores all active customers\")\n",
    "except Exception as e:\n",
    "    print(f\"Note: Dynamic table creation may require schema adjustments\")\n",
    "    print(f\"Error: {str(e)}\")"
   ],
   "id": "cell-39"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Inference with Sample Data"
   ],
   "id": "cell-40"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sample = X_test.head(10)\n",
    "test_sample_actual = y_test.head(10)\n",
    "\n",
    "test_predictions = full_pipeline.predict(test_sample)\n",
    "\n",
    "print(\"\\nSample Predictions:\")\n",
    "print(\"=\" * 70)\n",
    "for i, pred in enumerate(test_predictions):\n",
    "    actual = test_sample_actual.iloc[i]\n",
    "    error = pred - actual\n",
    "    error_pct = (error / actual * 100) if actual > 0 else 0\n",
    "    print(f\"Customer {i+1}: Predicted ${pred:>8.2f} | Actual ${actual:>8.2f} | Error ${error:>7.2f} ({error_pct:>6.1f}%)\")\n",
    "\n",
    "print(\"=\" * 70)"
   ],
   "id": "cell-41"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook accomplished:\n",
    "\n",
    "1. ✓ **Feature Engineering**: Created comprehensive RFM, behavioral, and velocity features\n",
    "2. ✓ **Model Training**: XGBoost with extensive hyperparameter tuning\n",
    "3. ✓ **Overfitting Prevention**: Multiple regularization techniques and cross-validation\n",
    "4. ✓ **Model Evaluation**: Comprehensive metrics across train/val/test sets\n",
    "5. ✓ **Deployment**: Registered to Snowflake Model Registry\n",
    "6. ✓ **Continuous Inference**: Dynamic tables for automated, incremental predictions\n",
    "\n",
    "**Key Insights**:\n",
    "- RFM metrics (Recency, Frequency, Monetary) are foundational predictors\n",
    "- Velocity indicators (30/90 day trends) capture momentum\n",
    "- Cohort comparisons provide context for individual performance\n",
    "- Dynamic tables enable near real-time CLV updates as customers transact\n",
    "\n",
    "**Next Steps**:\n",
    "- Monitor prediction accuracy on live data\n",
    "- Set up model retraining schedule (e.g., monthly)\n",
    "- Integrate predictions into CRM and marketing automation\n",
    "- A/B test CLV-based segmentation strategies"
   ],
   "id": "cell-42"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}