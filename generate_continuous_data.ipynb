{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Continuous Prediction Customer Data\n",
    "\n",
    "This notebook generates realistic data for customers with established transaction history.\n",
    "\n",
    "**Continuous Scenario**: Customers with 3+ months of activity and rich behavioral data.\n",
    "\n",
    "We'll generate:\n",
    "- 50,000 customers\n",
    "- 500,000+ raw transactions\n",
    "- Customer interactions and engagement events\n",
    "- Then derive RFM and other features from raw data"
   ],
   "id": "cell-0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "from snowflake.snowpark.context import get_active_session\n",
    "from snowflake.snowpark import functions as F\n",
    "\n",
    "# Get active Snowflake session\n",
    "session = get_active_session()\n",
    "\n",
    "random.seed(42)\n",
    "np.random.seed(42)"
   ],
   "id": "cell-1"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Set your database and schema here:"
   ],
   "id": "cell-2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Database and schema configuration\n",
    "DATABASE = 'ML_DEMO'\n",
    "SCHEMA = 'PUBLIC'\n",
    "\n",
    "# Set context\n",
    "session.use_database(DATABASE)\n",
    "session.use_schema(SCHEMA)\n",
    "\n",
    "print(f\"Using database: {DATABASE}\")\n",
    "print(f\"Using schema: {SCHEMA}\")\n",
    "print(f\"Current warehouse: {session.get_current_warehouse()}\")"
   ],
   "id": "cell-3"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Generation Parameters"
   ],
   "id": "cell-4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CUSTOMERS = 50000\n",
    "OBSERVATION_DATE = datetime(2024, 6, 30)\n",
    "MIN_HISTORY_DAYS = 90\n",
    "MAX_HISTORY_DAYS = 540"
   ],
   "id": "cell-5"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Customer Profiles"
   ],
   "id": "cell-4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_segments = ['high_value', 'medium_value', 'low_value', 'at_risk', 'churned']\n",
    "segment_weights = [0.15, 0.35, 0.30, 0.12, 0.08]\n",
    "\n",
    "customers = []\n",
    "\n",
    "for customer_id in range(1, NUM_CUSTOMERS + 1):\n",
    "    history_days = random.randint(MIN_HISTORY_DAYS, MAX_HISTORY_DAYS)\n",
    "    signup_date = OBSERVATION_DATE - timedelta(days=history_days)\n",
    "    \n",
    "    segment = np.random.choice(customer_segments, p=segment_weights)\n",
    "    \n",
    "    age_groups = ['18-24', '25-34', '35-44', '45-54', '55-64', '65+']\n",
    "    age_group = np.random.choice(age_groups, p=[0.10, 0.30, 0.27, 0.18, 0.10, 0.05])\n",
    "    \n",
    "    regions = ['Northeast', 'Southeast', 'Midwest', 'Southwest', 'West']\n",
    "    region = np.random.choice(regions)\n",
    "    \n",
    "    customers.append({\n",
    "        'CUSTOMER_ID': customer_id,\n",
    "        'SIGNUP_DATE': signup_date,\n",
    "        'AGE_GROUP': age_group,\n",
    "        'REGION': region,\n",
    "        'SEGMENT': segment,\n",
    "        'HISTORY_DAYS': history_days\n",
    "    })\n",
    "\n",
    "customers_df = pd.DataFrame(customers)\n",
    "print(f\"Generated {len(customers_df)} customer profiles\")\n",
    "customers_df.head()"
   ],
   "id": "cell-5"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Raw Transactions\n",
    "\n",
    "**Why raw transactions**: In real-world scenarios, we don't have pre-computed features. We start with transactional data and must derive meaningful metrics."
   ],
   "id": "cell-6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions = []\n",
    "\n",
    "product_categories = ['electronics', 'clothing', 'home_goods', 'beauty', 'sports', 'books', 'toys', 'grocery']\n",
    "\n",
    "for _, customer in customers_df.iterrows():\n",
    "    customer_id = customer['CUSTOMER_ID']\n",
    "    signup_date = customer['SIGNUP_DATE']\n",
    "    segment = customer['SEGMENT']\n",
    "    \n",
    "    if segment == 'high_value':\n",
    "        num_transactions = int(np.random.uniform(20, 60))\n",
    "        avg_amount = np.random.uniform(100, 300)\n",
    "        purchase_frequency_days = 7\n",
    "    elif segment == 'medium_value':\n",
    "        num_transactions = int(np.random.uniform(8, 25))\n",
    "        avg_amount = np.random.uniform(50, 120)\n",
    "        purchase_frequency_days = 15\n",
    "    elif segment == 'low_value':\n",
    "        num_transactions = int(np.random.uniform(3, 10))\n",
    "        avg_amount = np.random.uniform(20, 60)\n",
    "        purchase_frequency_days = 30\n",
    "    elif segment == 'at_risk':\n",
    "        num_transactions = int(np.random.uniform(5, 15))\n",
    "        avg_amount = np.random.uniform(40, 100)\n",
    "        purchase_frequency_days = 45\n",
    "    else:\n",
    "        num_transactions = int(np.random.uniform(1, 5))\n",
    "        avg_amount = np.random.uniform(25, 70)\n",
    "        purchase_frequency_days = 60\n",
    "    \n",
    "    current_date = signup_date + timedelta(days=random.randint(0, 7))\n",
    "    \n",
    "    for txn in range(num_transactions):\n",
    "        days_since_last = int(np.random.exponential(purchase_frequency_days))\n",
    "        current_date = current_date + timedelta(days=max(1, days_since_last))\n",
    "        \n",
    "        if current_date > OBSERVATION_DATE:\n",
    "            break\n",
    "        \n",
    "        amount = max(5, np.random.gamma(shape=2, scale=avg_amount/2))\n",
    "        amount = round(amount, 2)\n",
    "        \n",
    "        category = np.random.choice(product_categories)\n",
    "        quantity = int(np.random.poisson(lam=2) + 1)\n",
    "        \n",
    "        transactions.append({\n",
    "            'TRANSACTION_ID': len(transactions) + 1,\n",
    "            'CUSTOMER_ID': customer_id,\n",
    "            'TRANSACTION_DATE': current_date,\n",
    "            'AMOUNT': amount,\n",
    "            'PRODUCT_CATEGORY': category,\n",
    "            'QUANTITY': quantity\n",
    "        })\n",
    "\n",
    "transactions_df = pd.DataFrame(transactions)\n",
    "print(f\"Generated {len(transactions_df)} transactions\")\n",
    "transactions_df.head(10)"
   ],
   "id": "cell-7"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Customer Interaction Events"
   ],
   "id": "cell-8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorized transaction generation - much faster than loops\n",
    "print(\"Generating transactions (vectorized)...\")\n",
    "\n",
    "# Pre-calculate transaction counts per customer based on segment\n",
    "segment_txn_params = {\n",
    "    'high_value': {'min_txns': 20, 'max_txns': 60, 'min_amt': 100, 'max_amt': 300, 'freq_days': 7},\n",
    "    'medium_value': {'min_txns': 8, 'max_txns': 25, 'min_amt': 50, 'max_amt': 120, 'freq_days': 15},\n",
    "    'low_value': {'min_txns': 3, 'max_txns': 10, 'min_amt': 20, 'max_amt': 60, 'freq_days': 30},\n",
    "    'at_risk': {'min_txns': 5, 'max_txns': 15, 'min_amt': 40, 'max_amt': 100, 'freq_days': 45},\n",
    "    'churned': {'min_txns': 1, 'max_txns': 5, 'min_amt': 25, 'max_amt': 70, 'freq_days': 60}\n",
    "}\n",
    "\n",
    "# Calculate number of transactions per customer\n",
    "customers_df['num_txns'] = customers_df['SEGMENT'].apply(\n",
    "    lambda s: int(np.random.uniform(segment_txn_params[s]['min_txns'], \n",
    "                                     segment_txn_params[s]['max_txns']))\n",
    ")\n",
    "customers_df['avg_amount'] = customers_df['SEGMENT'].apply(\n",
    "    lambda s: np.random.uniform(segment_txn_params[s]['min_amt'], \n",
    "                                 segment_txn_params[s]['max_amt'])\n",
    ")\n",
    "customers_df['freq_days'] = customers_df['SEGMENT'].apply(\n",
    "    lambda s: segment_txn_params[s]['freq_days']\n",
    ")\n",
    "\n",
    "# Expand customer data to transaction level using np.repeat\n",
    "total_txns = customers_df['num_txns'].sum()\n",
    "customer_ids = np.repeat(customers_df['CUSTOMER_ID'].values, customers_df['num_txns'].values)\n",
    "signup_dates = np.repeat(customers_df['SIGNUP_DATE'].values, customers_df['num_txns'].values)\n",
    "avg_amounts = np.repeat(customers_df['avg_amount'].values, customers_df['num_txns'].values)\n",
    "freq_days = np.repeat(customers_df['freq_days'].values, customers_df['num_txns'].values)\n",
    "\n",
    "# Generate transaction dates using exponential distribution for realistic spacing\n",
    "transaction_dates = []\n",
    "current_idx = 0\n",
    "for _, customer in customers_df.iterrows():\n",
    "    num_txns = customer['num_txns']\n",
    "    signup = customer['SIGNUP_DATE']\n",
    "    frequency = customer['freq_days']\n",
    "    \n",
    "    # Start shortly after signup\n",
    "    current_date = signup + timedelta(days=random.randint(0, 7))\n",
    "    dates = []\n",
    "    \n",
    "    for _ in range(num_txns):\n",
    "        days_since_last = int(np.random.exponential(frequency))\n",
    "        current_date = current_date + timedelta(days=max(1, days_since_last))\n",
    "        if current_date > OBSERVATION_DATE:\n",
    "            break\n",
    "        dates.append(current_date)\n",
    "    \n",
    "    transaction_dates.extend(dates)\n",
    "    current_idx += num_txns\n",
    "\n",
    "# Truncate arrays to match actual dates generated (some may exceed OBSERVATION_DATE)\n",
    "actual_txn_count = len(transaction_dates)\n",
    "customer_ids = customer_ids[:actual_txn_count]\n",
    "avg_amounts = avg_amounts[:actual_txn_count]\n",
    "\n",
    "# Vectorized amount generation using gamma distribution\n",
    "amounts = np.maximum(5, np.random.gamma(shape=2, scale=avg_amounts/2, size=actual_txn_count))\n",
    "amounts = np.round(amounts, 2)\n",
    "\n",
    "# Vectorized category and quantity generation\n",
    "product_categories = ['electronics', 'clothing', 'home_goods', 'beauty', 'sports', 'books', 'toys', 'grocery']\n",
    "categories = np.random.choice(product_categories, size=actual_txn_count)\n",
    "quantities = np.random.poisson(lam=2, size=actual_txn_count) + 1\n",
    "\n",
    "# Create transactions dataframe\n",
    "transactions_df = pd.DataFrame({\n",
    "    'TRANSACTION_ID': range(1, actual_txn_count + 1),\n",
    "    'CUSTOMER_ID': customer_ids,\n",
    "    'TRANSACTION_DATE': transaction_dates,\n",
    "    'AMOUNT': amounts,\n",
    "    'PRODUCT_CATEGORY': categories,\n",
    "    'QUANTITY': quantities\n",
    "})\n",
    "\n",
    "print(f\"Generated {len(transactions_df)} transactions\")\n",
    "transactions_df.head(10)"
   ],
   "id": "cell-9"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derive RFM Features from Raw Transactions\n",
    "\n",
    "**RFM Metrics Explained**:\n",
    "- **Recency**: Days since last purchase - Recent customers are more likely to purchase again\n",
    "- **Frequency**: Number of purchases - Frequent buyers show loyalty and habit\n",
    "- **Monetary**: Total/average spend - High spenders have higher lifetime value potential\n",
    "\n",
    "These three metrics are fundamental because they capture:\n",
    "1. Current engagement (Recency)\n",
    "2. Behavioral patterns (Frequency)\n",
    "3. Economic value (Monetary)"
   ],
   "id": "cell-10"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorized interaction generation - much faster than loops\n",
    "print(\"Generating customer interactions (vectorized)...\")\n",
    "\n",
    "# Pre-calculate interaction counts per customer based on segment\n",
    "segment_interaction_params = {\n",
    "    'high_value': {'min_interactions': 100, 'max_interactions': 300},\n",
    "    'medium_value': {'min_interactions': 50, 'max_interactions': 120},\n",
    "    'low_value': {'min_interactions': 20, 'max_interactions': 60},\n",
    "    'at_risk': {'min_interactions': 15, 'max_interactions': 50},\n",
    "    'churned': {'min_interactions': 5, 'max_interactions': 20}\n",
    "}\n",
    "\n",
    "# Calculate number of interactions per customer\n",
    "customers_df['num_interactions'] = customers_df['SEGMENT'].apply(\n",
    "    lambda s: int(np.random.uniform(segment_interaction_params[s]['min_interactions'], \n",
    "                                     segment_interaction_params[s]['max_interactions']))\n",
    ")\n",
    "\n",
    "# Expand customer data to interaction level using np.repeat\n",
    "total_interactions = customers_df['num_interactions'].sum()\n",
    "customer_ids_int = np.repeat(customers_df['CUSTOMER_ID'].values, customers_df['num_interactions'].values)\n",
    "signup_dates_int = np.repeat(customers_df['SIGNUP_DATE'].values, customers_df['num_interactions'].values)\n",
    "history_days_int = np.repeat(customers_df['HISTORY_DAYS'].values, customers_df['num_interactions'].values)\n",
    "\n",
    "# Vectorized date generation - random offset within customer history\n",
    "days_offsets = np.random.randint(0, history_days_int + 1, size=total_interactions)\n",
    "\n",
    "# Convert to pandas Series for proper datetime arithmetic\n",
    "signup_series = pd.Series(signup_dates_int)\n",
    "event_dates = signup_series + pd.to_timedelta(days_offsets, unit='D')\n",
    "\n",
    "# Filter out dates beyond observation date\n",
    "valid_mask = event_dates <= pd.Timestamp(OBSERVATION_DATE)\n",
    "customer_ids_int = customer_ids_int[valid_mask]\n",
    "event_dates = event_dates[valid_mask].tolist()\n",
    "\n",
    "# Vectorized event type generation with realistic probabilities\n",
    "event_types = ['website_visit', 'email_open', 'email_click', 'support_ticket', 'product_view', 'cart_add']\n",
    "event_probs = [0.35, 0.20, 0.10, 0.05, 0.20, 0.10]\n",
    "actual_interaction_count = len(event_dates)\n",
    "selected_events = np.random.choice(event_types, size=actual_interaction_count, p=event_probs)\n",
    "\n",
    "# Create interactions dataframe\n",
    "interactions_df = pd.DataFrame({\n",
    "    'INTERACTION_ID': range(1, actual_interaction_count + 1),\n",
    "    'CUSTOMER_ID': customer_ids_int,\n",
    "    'EVENT_DATE': event_dates,\n",
    "    'EVENT_TYPE': selected_events\n",
    "})\n",
    "\n",
    "print(f\"Generated {len(interactions_df)} customer interactions\")\n",
    "interactions_df.head(10)"
   ],
   "id": "cell-11"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derive Purchase Pattern Features\n",
    "\n",
    "**Why these matter**:\n",
    "- **Inter-purchase time**: Identifies purchase rhythm and consistency\n",
    "- **Product diversity**: Customers who buy across categories tend to have higher engagement\n",
    "- **Average order value**: Indicates spending capacity per transaction\n",
    "- **Trend indicators**: Growing vs declining purchase patterns predict future behavior"
   ],
   "id": "cell-12"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "purchase_patterns = []\n",
    "\n",
    "for customer_id in customers_df['customer_id']:\n",
    "    cust_txns = transactions_df[transactions_df['customer_id'] == customer_id].sort_values('transaction_date')\n",
    "    \n",
    "    if len(cust_txns) > 1:\n",
    "        dates = pd.to_datetime(cust_txns['transaction_date'])\n",
    "        inter_purchase_times = dates.diff().dt.days.dropna()\n",
    "        avg_inter_purchase_days = inter_purchase_times.mean() if len(inter_purchase_times) > 0 else None\n",
    "        std_inter_purchase_days = inter_purchase_times.std() if len(inter_purchase_times) > 0 else None\n",
    "    else:\n",
    "        avg_inter_purchase_days = None\n",
    "        std_inter_purchase_days = None\n",
    "    \n",
    "    unique_categories = cust_txns['product_category'].nunique()\n",
    "    total_quantity = cust_txns['quantity'].sum()\n",
    "    \n",
    "    recent_30d_txns = cust_txns[cust_txns['transaction_date'] >= (OBSERVATION_DATE - timedelta(days=30))]\n",
    "    recent_30d_amount = recent_30d_txns['amount'].sum()\n",
    "    recent_30d_count = len(recent_30d_txns)\n",
    "    \n",
    "    recent_90d_txns = cust_txns[cust_txns['transaction_date'] >= (OBSERVATION_DATE - timedelta(days=90))]\n",
    "    recent_90d_amount = recent_90d_txns['amount'].sum()\n",
    "    recent_90d_count = len(recent_90d_txns)\n",
    "    \n",
    "    if len(cust_txns) >= 4:\n",
    "        mid_point = len(cust_txns) // 2\n",
    "        first_half_avg = cust_txns.iloc[:mid_point]['amount'].mean()\n",
    "        second_half_avg = cust_txns.iloc[mid_point:]['amount'].mean()\n",
    "        spending_trend = (second_half_avg - first_half_avg) / first_half_avg if first_half_avg > 0 else 0\n",
    "    else:\n",
    "        spending_trend = 0\n",
    "    \n",
    "    purchase_patterns.append({\n",
    "        'customer_id': customer_id,\n",
    "        'avg_inter_purchase_days': avg_inter_purchase_days,\n",
    "        'std_inter_purchase_days': std_inter_purchase_days,\n",
    "        'unique_categories_purchased': unique_categories,\n",
    "        'total_items_purchased': total_quantity,\n",
    "        'recent_30d_amount': recent_30d_amount,\n",
    "        'recent_30d_count': recent_30d_count,\n",
    "        'recent_90d_amount': recent_90d_amount,\n",
    "        'recent_90d_count': recent_90d_count,\n",
    "        'spending_trend': spending_trend\n",
    "    })\n",
    "\n",
    "purchase_patterns_df = pd.DataFrame(purchase_patterns)\n",
    "print(\"Purchase pattern features:\")\n",
    "purchase_patterns_df.head(10)"
   ],
   "id": "cell-13"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derive Engagement Features from Interactions\n",
    "\n",
    "**Why these matter**: Engagement beyond purchases indicates interest and intent. High engagement without recent purchases may signal opportunity or friction."
   ],
   "id": "cell-14"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engagement_features = interactions_df.groupby('customer_id').agg(\n",
    "    total_interactions=('interaction_id', 'count'),\n",
    "    website_visits=('event_type', lambda x: (x == 'website_visit').sum()),\n",
    "    email_opens=('event_type', lambda x: (x == 'email_open').sum()),\n",
    "    email_clicks=('event_type', lambda x: (x == 'email_click').sum()),\n",
    "    support_tickets=('event_type', lambda x: (x == 'support_ticket').sum()),\n",
    "    product_views=('event_type', lambda x: (x == 'product_view').sum()),\n",
    "    cart_adds=('event_type', lambda x: (x == 'cart_add').sum())\n",
    ").reset_index()\n",
    "\n",
    "engagement_features['email_engagement_rate'] = (\n",
    "    engagement_features['email_clicks'] / engagement_features['email_opens'].replace(0, np.nan)\n",
    ").fillna(0)\n",
    "\n",
    "print(\"Engagement features derived from interactions:\")\n",
    "engagement_features.head(10)"
   ],
   "id": "cell-15"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine All Features into Final Dataset"
   ],
   "id": "cell-16"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = customers_df.merge(rfm_features, on='customer_id', how='left')\n",
    "final_df = final_df.merge(purchase_patterns_df, on='customer_id', how='left')\n",
    "final_df = final_df.merge(engagement_features, on='customer_id', how='left')\n",
    "\n",
    "final_df.fillna(0, inplace=True)\n",
    "\n",
    "print(f\"Final dataset shape: {final_df.shape}\")\n",
    "final_df.head()"
   ],
   "id": "cell-17"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Target Variable: Forward-Looking 12-Month CLV\n",
    "\n",
    "**Why this is the label**: We predict future value, not historical. The model learns patterns that indicate future spending behavior."
   ],
   "id": "cell-18"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_future_ltv(row):\n",
    "    base_ltv = row['monetary_total'] * 0.6\n",
    "    \n",
    "    recency_factor = max(0.5, 1.5 - (row['recency_days'] / 180))\n",
    "    frequency_factor = min(2.0, 1 + (row['frequency'] / 20))\n",
    "    \n",
    "    engagement_factor = 1 + (row['total_interactions'] / 500)\n",
    "    \n",
    "    trend_factor = 1 + row['spending_trend']\n",
    "    trend_factor = max(0.5, min(2.0, trend_factor))\n",
    "    \n",
    "    future_ltv = base_ltv * recency_factor * frequency_factor * engagement_factor * trend_factor\n",
    "    \n",
    "    future_ltv = future_ltv * np.random.uniform(0.7, 1.3)\n",
    "    \n",
    "    return round(max(0, future_ltv), 2)\n",
    "\n",
    "final_df['future_12m_ltv'] = final_df.apply(calculate_future_ltv, axis=1)\n",
    "\n",
    "print(f\"Average future 12-month LTV: ${final_df['future_12m_ltv'].mean():.2f}\")\n",
    "print(f\"Median future 12-month LTV: ${final_df['future_12m_ltv'].median():.2f}\")"
   ],
   "id": "cell-19"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Quality Checks"
   ],
   "id": "cell-20"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save All Datasets to Snowflake"
   ],
   "id": "cell-21"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uppercase all column names before saving to Snowflake\n",
    "customers_df.columns = customers_df.columns.str.upper()\n",
    "transactions_df.columns = transactions_df.columns.str.upper()\n",
    "interactions_df.columns = interactions_df.columns.str.upper()\n",
    "final_df.columns = final_df.columns.str.upper()\n",
    "\n",
    "# Save customers profile\n",
    "customers_sf_df = session.create_dataframe(customers_df)\n",
    "customers_sf_df.write.mode('overwrite').save_as_table('CONTINUOUS_CUSTOMERS_PROFILE')\n",
    "print(f\"✓ Saved CONTINUOUS_CUSTOMERS_PROFILE: {customers_sf_df.count()} rows\")\n",
    "\n",
    "# Save transactions\n",
    "transactions_sf_df = session.create_dataframe(transactions_df)\n",
    "transactions_sf_df.write.mode('overwrite').save_as_table('CONTINUOUS_TRANSACTIONS')\n",
    "print(f\"✓ Saved CONTINUOUS_TRANSACTIONS: {transactions_sf_df.count()} rows\")\n",
    "\n",
    "# Save interactions\n",
    "interactions_sf_df = session.create_dataframe(interactions_df)\n",
    "interactions_sf_df.write.mode('overwrite').save_as_table('CONTINUOUS_INTERACTIONS')\n",
    "print(f\"✓ Saved CONTINUOUS_INTERACTIONS: {interactions_sf_df.count()} rows\")\n",
    "\n",
    "# Save final feature dataset\n",
    "features_sf_df = session.create_dataframe(final_df)\n",
    "features_sf_df.write.mode('overwrite').save_as_table('CONTINUOUS_CUSTOMERS_FEATURES')\n",
    "print(f\"✓ Saved CONTINUOUS_CUSTOMERS_FEATURES: {features_sf_df.count()} rows\")\n",
    "\n",
    "print(f\"\\n✓ All datasets saved to {DATABASE}.{SCHEMA}\")"
   ],
   "id": "cell-22"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify Tables in Snowflake"
   ],
   "id": "cell-25"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show table information\n",
    "tables = session.sql(f\"\"\"\n",
    "    SELECT table_name, row_count \n",
    "    FROM {DATABASE}.INFORMATION_SCHEMA.TABLES \n",
    "    WHERE table_schema = '{SCHEMA}' \n",
    "    AND table_name LIKE 'CONTINUOUS%'\n",
    "    ORDER BY table_name\n",
    "\"\"\").to_pandas()\n",
    "\n",
    "print(\"\\nCreated tables:\")\n",
    "print(tables)\n",
    "\n",
    "# Display sample from features table\n",
    "print(\"\\nSample data from CONTINUOUS_CUSTOMERS_FEATURES:\")\n",
    "session.table('CONTINUOUS_CUSTOMERS_FEATURES').limit(10).show()"
   ],
   "id": "cell-26"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}